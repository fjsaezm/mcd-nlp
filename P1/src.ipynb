{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a19d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('opinion_lexicon')\n",
    "\n",
    "# Assignment 2\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Assignment 3\n",
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166aa33",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "### Task 1.1 \n",
    "**Loading all the hotel reviews from the Yelp hotel reviews file.**\n",
    "\n",
    "### Task 1.2 (optional) \n",
    "\n",
    "**Loading line by line the reviews from the Yelp beauty/spa resorts and restaurants reviews files.**\n",
    "\n",
    "### Task 1.3 (optional) \n",
    "\n",
    "**Loading line by line reviews on other domains (e.g., movies, books, phones, digital music, CDs and videogames) from McAuleyâ€™s Amazon dataset.**\n",
    "\n",
    "We tackle all of these tasks at the same time since a general enough functions solves all of them directly. The function `load_json_line_by_line()` reads a json file line by line and returns the dataset built.\n",
    "\n",
    "We additionaly created a test function that tests the loading of all the described datasets. We have selected the Amazon cell phones and accesories dataset because it is big enough without being huge and we also have the required aspects for it (see tasks 2.1 to 2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b66b516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file inputs/yelp_dataset/yelp_hotels.json\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inputs/yelp_dataset/yelp_hotels.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m reviews loaded\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(reviews)))\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExample review: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(reviews[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtest_load_json_line_by_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mtest_load_json_line_by_line\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m---> 23\u001b[0m     reviews \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_line_by_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m reviews loaded\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(reviews)))\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExample review: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(reviews[\u001b[38;5;241m0\u001b[39m]))\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mload_json_line_by_line\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json_line_by_line\u001b[39m(path):\n\u001b[1;32m      4\u001b[0m     reviews \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;66;03m# If line ends with a coma, remove it.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inputs/yelp_dataset/yelp_hotels.json'"
     ]
    }
   ],
   "source": [
    "hotels_path = 'inputs/yelp_dataset/yelp_hotels.json'\n",
    "\n",
    "def load_json_line_by_line(path):\n",
    "    reviews = []\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # If line ends with a coma, remove it.\n",
    "            if len(line) > 2:\n",
    "                line = line[:-2] if line[-2] == ',' else line[:-1]\n",
    "                reviews.append(json.loads(line))\n",
    "    return reviews\n",
    "\n",
    "def test_load_json_line_by_line():\n",
    "    paths = [\n",
    "        'inputs/yelp_dataset/yelp_hotels.json',\n",
    "        'inputs/yelp_dataset/yelp_beauty_spas.json',\n",
    "        'inputs/yelp_dataset/yelp_restaurants.json',\n",
    "        'inputs/amazon/Cell_Phones_and_Accessories_5.json'\n",
    "    ]\n",
    "    \n",
    "    for path in paths:\n",
    "        print('Reading file {}\\n'.format(path))\n",
    "        reviews = load_json_line_by_line(path)\n",
    "        print('{} reviews loaded\\n'.format(len(reviews)))\n",
    "        print('Example review: {}\\n'.format(reviews[0]))\n",
    "\n",
    "test_load_json_line_by_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c01a26",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "### Task 2.1\n",
    "\n",
    "**Loading (and printing on screen) the vocabulary of the `aspects_hotels.csv` file, and directly using it to identify aspect references in the reviews. In particular, the aspects terms could be mapped by exact matching with nouns appearing in the reviews.**\n",
    "\n",
    "We will compute a dictionary that matches a certain aspect to every word related to it. It will usually be called `aspect_words_dict`. This will optimize knowing which aspect is related to each word.\n",
    "\n",
    "The function `build_simple_vocab` creates this dictionary given a path to the file with the initial vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06369e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aspect_hotels_path = 'inputs/aspects/aspects_hotels.csv'\n",
    "\n",
    "def load_vocab(path):\n",
    "    vocab = pd.read_csv(path,names = ['aspect', 'word'])\n",
    "    return vocab.groupby('aspect')['word'].apply(list)\n",
    "\n",
    "def create_word_to_aspects_dict(aspect_words_dict):\n",
    "    '''\n",
    "        This function transforms an 'aspect to words'\n",
    "        dictionary to a 'word to aspect' default dictionary\n",
    "    '''\n",
    "    word_aspect_dict = defaultdict(str)\n",
    "    for aspect, words in aspect_words_dict.items():\n",
    "        for word in words:\n",
    "            word_aspect_dict[word] = aspect\n",
    "    return word_aspect_dict\n",
    "    \n",
    "def build_simple_vocab(path):\n",
    "    aspect_words_dict = load_vocab(path)\n",
    "    word_aspect_dict = create_word_to_aspects_dict(aspect_words_dict)\n",
    "    return word_aspect_dict\n",
    "\n",
    "build_simple_vocab(aspect_hotels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebd488",
   "metadata": {},
   "source": [
    "In the following cells we compute the aspects referenced by each review and display the result for the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdd61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vocabulary(text):\n",
    "    '''\n",
    "        Returns a list of the words from\n",
    "        the given text.\n",
    "    '''\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Obtain the words \n",
    "    return [w.lower() for w in tokens if w.isalnum()]\n",
    "\n",
    "def find_aspects_in_reviews(reviews, word_aspect_dict):\n",
    "    '''\n",
    "        Given a list of reviews, returns a set with the\n",
    "        aspects that each review references.\n",
    "    '''\n",
    "    # Initialize references list\n",
    "    refered_aspects = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        # Obtain review vocabulary\n",
    "        r_vocab = get_text_vocabulary(review['reviewText'])\n",
    "\n",
    "        # Search for words related to aspects and append\n",
    "        # to vector if word appears\n",
    "        refered_aspects.append(set([\n",
    "            word_aspect_dict[word] for word in r_vocab if word_aspect_dict[word] != ''\n",
    "        ]))\n",
    "\n",
    "    return refered_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_reviews = load_json_line_by_line(hotels_path)\n",
    "word_aspect_dict = build_simple_vocab(aspect_hotels_path)\n",
    "refered_aspects = find_aspects_in_reviews(hotel_reviews, word_aspect_dict)\n",
    "\n",
    "for review, aspects in zip(hotel_reviews[:3], refered_aspects[:3]):\n",
    "    print('\\tReview: {} \\n\\tAspects: {} \\n'.format(review['reviewText'], aspects))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb5086",
   "metadata": {},
   "source": [
    "### Task 2.2 (optional)\n",
    "\n",
    "**Generating or extending the lists of terms of each aspect with synonyms extracted from WordNet.**\n",
    "\n",
    "For this second task we expand the vocabulty using synonims extracted from Wordnet. The function `build_vocab()` is analogous to the previous `build_simple_vocab()` but takes this synonims into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e87059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_word_synonims(word):\n",
    "    syns = []\n",
    "    for syn in wn.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            syns.append(l.name())\n",
    "    return syns\n",
    "\n",
    "def add_synonims_to_vocab(word_aspect_dict):\n",
    "    '''\n",
    "        Given a vocabulary, extends it by adding the synonims\n",
    "        of each word.\n",
    "    '''\n",
    "    result = copy.deepcopy(word_aspect_dict)\n",
    "    for word, aspect in word_aspect_dict.items():\n",
    "        for synonym in get_word_synonims(word):\n",
    "            result[synonym] = aspect\n",
    "    return result\n",
    "\n",
    "def build_vocab(path):\n",
    "    '''\n",
    "        Builds a vocabulary taking synonims into account.\n",
    "    '''\n",
    "    aspect_words_dict = load_vocab(path)\n",
    "    word_aspect_dict = create_word_to_aspects_dict(aspect_words_dict)\n",
    "    word_aspect_dict_extended = add_synonims_to_vocab(word_aspect_dict)\n",
    "    return word_aspect_dict_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5043e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_expand_vocab(reviews_path, aspects_path,\n",
    "                      n_displayed_reviews=3):\n",
    "    '''\n",
    "        This functions displayes the refered aspects in the first\n",
    "        n_displayed_reviews reviews using both vocabularies\n",
    "    '''\n",
    "    reviews = load_json_line_by_line(reviews_path)\n",
    "    \n",
    "    simple_vocab = build_simple_vocab(aspects_path)\n",
    "    complex_vocab = build_vocab(aspects_path)\n",
    "\n",
    "    refered_aspects = find_aspects_in_reviews(\n",
    "        reviews[:n_displayed_reviews], simple_vocab)\n",
    "    refered_aspects_extended = find_aspects_in_reviews(\n",
    "        reviews[:n_displayed_reviews], complex_vocab)\n",
    "    \n",
    "    for review, aspects, extended_aspects in zip(reviews[:n_displayed_reviews],\n",
    "                                                 refered_aspects,\n",
    "                                                 refered_aspects_extended):\n",
    "        print('\\tReview: {} \\n\\tAspects: {} \\n\\tExtended Vocab Aspects: {} \\n'.format(\n",
    "            review['reviewText'], aspects, extended_aspects))\n",
    "        \n",
    "    return simple_vocab, complex_vocab\n",
    "\n",
    "_ = test_expand_vocab(hotels_path, aspect_hotels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e988a23",
   "metadata": {},
   "source": [
    "### Task 2.3 (optional)\n",
    "\n",
    "**Managing vocabularies for additional Yelp or Amazon domains. See assignments 1.2 and 1.3**\n",
    "\n",
    "Extended our previous functions to the new datasets is trivial. We simple need to load the correct aspects for each review. The following test function computes the following for the Yelp hotels, Yelp restaurants and Amazon phones datasets:\n",
    "\n",
    "- Load the reviews and build both the simple and complex vocabularies.\n",
    "- Print the aspects found in the first few reviews with each vocabulary.\n",
    "- Print the number of words in both the simple and extended vocabulary for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a67728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_build_vocabulary(n_displayed_reviews=1):\n",
    "    test_case_names = [\n",
    "        'Yelp hotels',\n",
    "        'Yelp restaurants',\n",
    "        'Amazon cell phones and accesories'\n",
    "    ]\n",
    "    reviews_paths = [\n",
    "        'inputs/yelp_dataset/yelp_hotels.json',\n",
    "        'inputs/yelp_dataset/yelp_restaurants.json',\n",
    "        'inputs/amazon/Cell_Phones_and_Accessories_5.json'\n",
    "    ]\n",
    "    aspects_paths = [\n",
    "        'inputs/aspects/aspects_hotels.csv',\n",
    "        'inputs/aspects/aspects_restaurants.csv',\n",
    "        'inputs/aspects/aspects_phones.csv'\n",
    "    ]\n",
    "    \n",
    "    for name, reviews_path, aspects_path in \\\n",
    "            zip(test_case_names, reviews_paths, aspects_paths):\n",
    "        print('----- {} dataset -----\\n'.format(name))\n",
    "        \n",
    "        simple_vocab, complex_vocab = test_expand_vocab(\n",
    "            reviews_path, aspects_path, n_displayed_reviews=n_displayed_reviews)\n",
    "        \n",
    "        print('Words in simple vocab: {}'.format(len(simple_vocab.keys())))\n",
    "        print('Words in complex vocab: {}\\n'.format(len(complex_vocab.keys())))\n",
    "\n",
    "test_build_vocabulary(n_displayed_reviews=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fccbc",
   "metadata": {},
   "source": [
    "### TODOS\n",
    "\n",
    "- Hacer el 2.4\n",
    "- En el 2.2, meter hiponimos e hipernonimos a parte de sinonimos?\n",
    "- Utilizar spaCy para negaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b719b",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d457dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list=set(opinion_lexicon.positive())\n",
    "neg_list=set(opinion_lexicon.negative())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a7bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_polarity = defaultdict(str)\n",
    "for word in pos_list:\n",
    "    all_polarity[word] = 'positive'\n",
    "for word in neg_list:\n",
    "    all_polarity[word] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84afd93b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mall_polarity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "vocab_polarity = defaultdict(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb584ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
